{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test preprocess file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preproces  import DataProcessor\n",
    "data_processor = DataProcessor('snappfood/train.csv', 'snappfood/test.csv')\n",
    "data_processor.process()      \n",
    "train_dataloader = data_processor.dataloader_train\n",
    "test_dataloader = data_processor.dataloader_tst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "Extractor_module = importlib.import_module('Extractor')\n",
    "importlib.reload(Extractor_module)\n",
    "from Extractor import BERTFeatureExtractor,SimpleNN, ModelTrainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "lable_tensor = data_processor.labels_tensor_train\n",
    "input_train = data_processor.input_train\n",
    "feature_extractor = BERTFeatureExtractor()\n",
    "features_cls = feature_extractor.extract_features_cls(input_train[0:10])\n",
    "features_4_layers = feature_extractor.extract_features_4_layers(input_train[0:10])\n",
    "\n",
    "# Initialize models, loss functions, and optimizers\n",
    "model_cls = SimpleNN(input_size=768, num_classes=2)\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "optimizer_cls = torch.optim.Adam(model_cls.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "model_4_layers = SimpleNN(input_size=768 , num_classes=2)  # Adjust input size for concatenated features\n",
    "criterion_4_layers = nn.CrossEntropyLoss()\n",
    "optimizer_4_layers = torch.optim.Adam(model_4_layers.parameters(), lr=0.001)\n",
    "\n",
    "# Train the models\n",
    "trainer_cls = ModelTrainer(model_cls, criterion_cls, optimizer_cls)\n",
    "trainer_cls.train(features_cls, lable_tensor[0:10])\n",
    "\n",
    "trainer_4_layers = ModelTrainer(model_4_layers, criterion_4_layers, optimizer_4_layers)\n",
    "trainer_4_layers.train(features_4_layers, lable_tensor[0:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test Set_final_layer_Bert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Set_final_layer_module = importlib.import_module('Set_final_layer_Bert')\n",
    "from Set_final_layer_Bert import BERTClassifier\n",
    "importlib.reload(Set_final_layer_module)\n",
    "# Create an instance of BERTClassifier\n",
    "bert_classifier = Set_final_layer_module.BERTClassifier()\n",
    "# Train the model\n",
    "bert_classifier.train(train_dataloader, num_epochs=3)\n",
    "bert_classifier.evaluate(test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test Set_all_Layers_Bert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Set_all_layer_module = importlib.import_module('set_all_layers_Bert')\n",
    "from set_all_layers_Bert import Set_all_layers\n",
    "importlib.reload(Set_all_layer_module)\n",
    "# Create an instance of BERTSentimentAnalyzer\n",
    "Set_all_layers_instance = Set_all_layer_module.Set_all_layers()\n",
    "# Train the model\n",
    "Set_all_layers_instance.train(train_dataloader)\n",
    "Set_all_layers_instance.evaluate(test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
